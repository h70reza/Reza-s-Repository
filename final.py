# -*- coding: utf-8 -*-
"""Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a2mbff5RLqOpuy1jeOqas-eiHYAAJswE
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import Ridge
from sklearn.model_selection import ShuffleSplit
from scipy.stats import zscore
from sklearn.model_selection import cross_validate
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import make_scorer
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import tensorflow as tf

from google.colab import drive

drive.mount('40192_2013_16_MOESM1_ESM.csv',force_remount=True)

df=pd.read_csv('/content/40192_2013_16_MOESM1_ESM.csv/MyDrive/Machine Learning/40192_2013_16_MOESM1_ESM.csv')

pd.set_option('display.max_columns',None)
df.head()

"""* Conduct EDA and summarize your observations. """

df.drop('Sl. No.',axis=1, inplace=True)
sns.set(rc={'figure.figsize':(15,10)})
sns.heatmap(df.isnull())

df.describe()

df.info()

"""### We see that there is no null input data"""

# function to identify optimum number of clusters
def optimize_k_means(data,max_k):
  means=[]
  inertias=[]

  for k in range(1,max_k):
    kmeans=KMeans(n_clusters=k,random_state=42)
    kmeans.fit(data)

    means.append(k)
    inertias.append(kmeans.inertia_)

    # elbow plot
  fig=plt.subplots(figsize=(10,5))
  plt.plot(means,inertias, 'o-')
  plt.xlabel('Number of Clusters')
  plt.ylabel('Inertia')
  plt.grid(True)
  plt.show()

optimize_k_means(df,10)

"""### we set the number of clusters to 3"""

kmeans=KMeans(n_clusters=3,random_state=42)

kmeans.fit(df)

df['kmeans_3']=kmeans.labels_
df

plt.scatter(x=df['RedRatio'],y=df['Fatigue'],c=df['kmeans_3'],cmap='viridis')
plt.xlim([0,2000])

df.drop('kmeans_3',axis=1,inplace=True)

corr=df.corr()
corr.style.background_gradient(cmap='coolwarm').set_properties(**{'font-size': '0pt'})
mask = np.zeros_like(corr, dtype=bool)
mask[np.triu_indices_from(mask)] = True
corr[mask] = np.nan
(corr
 .style
 .background_gradient(cmap='coolwarm', axis=None, vmin=-1, vmax=1)
 .highlight_null(null_color='#f1f1f1')  # Color NaNs grey
 .set_precision(2))

X=df.drop(['Fatigue'],axis=1)
columns=X.columns
y=df['Fatigue']
X_int, X_test, y_int, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scoring Functions
def MAE(y_true, y_pred):
  return (1/len(y_true)) * np.sum(np.abs((y_true-y_pred)))

def MAEf(y_true, y_pred):
  return (1/len(y_true)) * np.sum(np.abs((y_true-y_pred) / y_true))

def RMSE(y_true, y_pred):
  return np.sqrt(  (1/len(y_true)) * np.sum( ((y_true-y_pred))**2)  )

def RMSEf(y_true, y_pred):
  return np.sqrt(  (1/len(y_true)) * np.sum( ((y_true-y_pred) / y_true)**2)  )

def SDE(y_true, y_pred):
  e = (1/len(y_true)) * np.sum(np.abs((y_true-y_pred)))
  return np.sqrt(  (1/len(y_true)) * np.sum( np.abs(((y_true-y_pred)) - e)**2  ) )

def SDEf(y_true, y_pred):
  ef = (1/len(y_true)) * np.sum(np.abs((y_true-y_pred) / y_true))
  return np.sqrt(  (1/len(y_true)) * np.sum( np.abs(((y_true-y_pred) / y_true) - ef)**2  ) )

def R2(y_true, y_pred):
  n = np.sum( (y_true-np.mean(y_true) ) * (y_pred-np.mean(y_pred)) )
  d = np.sqrt(  np.sum( (y_true-np.mean(y_true))**2 ) * np.sum( (y_pred-np.mean(y_pred))**2 )   )
  if d != 0:
    return (float(n)/float(d))**2
  else: 
    return 0 

scoring_metrics = {'R2': make_scorer(R2,greater_is_better=True),
                  'MAEf': make_scorer(MAEf,greater_is_better=True),
                  'RMSEf': make_scorer(RMSEf,greater_is_better=True),
                  'SDEf': make_scorer(SDEf,greater_is_better=True),
                  'MAE': make_scorer(MAE,greater_is_better=True),
                  'RMSE': make_scorer(RMSE,greater_is_better=True),
                  'SDE': make_scorer(SDE,greater_is_better=True),
                 }

skf=6
def avr_cross_val(estim,X,y,scoring=scoring_metrics,cv=skf):
  score=cross_validate(estim,X,y,scoring=scoring_metrics,cv=skf)
  return {metric: round(np.mean(scores), 5) for metric, scores in score.items()}

lin_reg=LinearRegression()
linreg_cross=avr_cross_val(lin_reg,X_int,y_int,scoring=scoring_metrics)

linreg_cross

linreg_pred=cross_val_predict(lin_reg,X_test,y_test)

knn=KNeighborsClassifier(n_neighbors=2)
knn_cross=avr_cross_val(knn,X_int,y_int,scoring=scoring_metrics,cv=6)

knn_cross

knn_pred=cross_val_predict(knn,X_test,y_test,cv=2)

rig=Ridge()
rig_cross=avr_cross_val(rig,X_int,y_int,scoring=scoring_metrics)

rig_cross

rig_pred=cross_val_predict(rig,X_test,y_test)

ann_dp_wd=tf.keras.models.Sequential()
ann_dp_wd.add(tf.keras.layers.Dense(units=150,input_dim=25,kernel_initializer="normal",activation="relu"))
ann_dp_wd.add(tf.keras.layers.Dense(units=150,kernel_initializer="normal",activation="relu"))
ann_dp_wd.add(tf.keras.layers.Dense(units=150,kernel_initializer="normal",activation="relu"))
ann_dp_wd.add(tf.keras.layers.Dense(units=150,kernel_initializer="normal",activation="relu"))
ann_dp_wd.add(tf.keras.layers.Dense(units=1,kernel_initializer="normal",activation="linear"))
ann_dp_wd.compile(optimizer="adam",loss="mean_squared_error")
fit_dp_wd=ann_dp_wd.fit(X_int.values,y_int.values,batch_size=15,epochs=150)

ann_pred = np.reshape(ann_dp_wd.predict(X_test),y_test.shape)

print('RMSE=',np.sqrt(mean_squared_error(y_test,ann_pred)))
print('MAE=',mean_absolute_error(y_test,ann_pred))
print('R2=',r2_score(y_test,ann_pred))
print('MAEf=',(1/len(y_test)) * np.sum(np.abs((y_test-ann_pred) / y_test)))
print('RMSEf',np.sqrt(  (1/len(y_test)) * np.sum( ((y_test-ann_pred) / y_test)**2)))
print('SDE=',np.sqrt(  (1/len(y_test)) * np.sum( np.abs(((y_test-ann_pred)) - (1/len(y_test)) * np.sum(np.abs((y_test-ann_pred))))**2  ) ))
print('SDEf=',np.sqrt(  (1/len(y_test)) * np.sum( np.abs(((y_test-ann_pred) / y_test) - (1/len(y_test)) * np.sum(np.abs((y_test-ann_pred) / y_test)))**2  ) ))

plt.subplot(2,2,1)
plt.scatter(y_test,rig_pred)
plt.plot(y_test,y_test)
plt.title('Ridge_Regression',fontdict={'fontsize': 18})
plt.ylabel('Predicted')
plt.subplot(2,2,2)
plt.scatter(y_test,knn_pred)
plt.plot(y_test,y_test)
plt.title('KNN',fontdict={'fontsize': 18})
plt.subplot(2,2,3)
plt.scatter(y_test,ann_pred)
plt.plot(y_test,y_test)
plt.title('ANN',fontdict={'fontsize': 18})
plt.ylabel('predicted')
plt.xlabel('True')
plt.subplot(2,2,4)
plt.scatter(y_test,linreg_pred)
plt.plot(y_test,y_test)
plt.title('Linear Regression',fontdict={'fontsize': 18})
plt.xlabel('True')

